{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = pd.read_csv(\"train.csv\")\n",
    "TESTING = pd.read_csv(\"test.csv\") # Without Label\n",
    "AUGMENTED = pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\") # Additional data to augment training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID columns\n",
    "TRAINING = TRAINING.drop(columns=['id'])\n",
    "\n",
    "# Drop duplplicates if it exists\n",
    "TRAINING = TRAINING.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "AUGMENTED = AUGMENTED.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "# Make Attrition for Augmented be 0 and 1\n",
    "AUGMENTED['Attrition'] = AUGMENTED['Attrition'].replace({'Yes':1,'No':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropOneDim(data: pd.DataFrame, Label: str) -> (pd.DataFrame, list):\n",
    "    one_dimensional_cols = [col for col in data.columns if data[col].nunique() == 1]\n",
    "    print(\"Dropped columns:\", one_dimensional_cols)  # Print dropped columns\n",
    "\n",
    "    # Filter out one-dimensional columns\n",
    "    remaining_columns = [col for col in data.columns if col not in one_dimensional_cols]\n",
    "    \n",
    "    # Exclude the label column from remaining columns\n",
    "    remaining_columns = [col for col in remaining_columns if col != Label]\n",
    "\n",
    "    # Return updated DataFrame and remaining columns\n",
    "    return data[remaining_columns + [Label]], remaining_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['EmployeeCount', 'Over18', 'StandardHours']\n",
      "Dropped columns: ['EmployeeCount', 'Over18', 'StandardHours']\n"
     ]
    }
   ],
   "source": [
    "# Drop one-dimensional features\n",
    "TRAINING, remaining_columns = dropOneDim(TRAINING, \"Attrition\")\n",
    "TESTING = TESTING[remaining_columns]\n",
    "AUGMENTED, remaining_columns = dropOneDim(AUGMENTED, \"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDER = [\"Non-Travel\", \"Travel_Rarely\", \"Travel_Frequently\"]\n",
    "TRAINING[\"BusinessTravel\"] = pd.Categorical(TRAINING[\"BusinessTravel\"], categories=ORDER, ordered=True)\n",
    "AUGMENTED[\"BusinessTravel\"] = pd.Categorical(AUGMENTED[\"BusinessTravel\"], categories=ORDER, ordered=True)\n",
    "TESTING[\"BusinessTravel\"] = pd.Categorical(TESTING[\"BusinessTravel\"], categories=ORDER, ordered=True)\n",
    "# Encode as integers\n",
    "TRAINING[\"BusinessTravel\"] = TRAINING[\"BusinessTravel\"].cat.codes.replace(-1, None)\n",
    "AUGMENTED[\"BusinessTravel\"] = AUGMENTED[\"BusinessTravel\"].cat.codes.replace(-1, None)\n",
    "TESTING[\"BusinessTravel\"] = TESTING[\"BusinessTravel\"].cat.codes.replace(-1, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\n",
      "Numeric Features: ['Age', 'BusinessTravel', 'DailyRate', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'Attrition']\n"
     ]
    }
   ],
   "source": [
    "NUMERICS = TRAINING.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "CATEGORICALS = TRAINING.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "print (f'Categorical features: {CATEGORICALS}')\n",
    "print (f'Numeric Features: {NUMERICS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_median(df, exclude_columns=[]):\n",
    "    numeric_features = df.select_dtypes(include=['number']).columns\n",
    "    numeric_features = [col for col in numeric_features if col not in exclude_columns]\n",
    "    for column in numeric_features:\n",
    "        median_value = df[column].median()\n",
    "        df[column].fillna(median_value, inplace=True)\n",
    "\n",
    "# Apply on df\n",
    "impute_median(TRAINING, exclude_columns=['Attrition'])\n",
    "impute_median(TESTING)\n",
    "impute_median(AUGMENTED, exclude_columns=['Attrition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(df, exclude_columns=[], lower_quantile=0.01, upper_quantile=0.99):\n",
    "    numeric_features = df.select_dtypes(include=['number']).columns\n",
    "    numeric_features = [col for col in numeric_features if col not in exclude_columns]\n",
    "    for column in numeric_features:\n",
    "        lower_bound = df[column].quantile(lower_quantile)\n",
    "        upper_bound = df[column].quantile(upper_quantile)\n",
    "        df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Apply on DF\n",
    "cap_outliers(TRAINING, exclude_columns=['Label'])\n",
    "cap_outliers(TESTING)\n",
    "cap_outliers(AUGMENTED, exclude_columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Employee number from Augmented dataframe\n",
    "AUGMENTED = AUGMENTED.drop(columns=['EmployeeNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive new features\n",
    "def feature_extraction(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # df['AgeGroup'] = pd.cut(df['Age'], bins=[20, 30, 40, 50], labels=['20-30', '31-40', '41-50'])\n",
    "    df['Tenure'] = df['Age'] - df['YearsAtCompany']\n",
    "    df['IncomePerYear'] = df['MonthlyIncome'] / df['YearsAtCompany']\n",
    "    df['JobSatisfactionRatio'] = df['JobSatisfaction'] / df['YearsAtCompany']\n",
    "    # df['DistanceCategory'] = pd.cut(df['DistanceFromHome'], bins=[0, 10, 20, 30], labels=['0-10 km', '11-20 km', '21-30 km'])\n",
    "    df['TotalTrainingTime'] = df['TrainingTimesLastYear'] * df['YearsAtCompany']\n",
    "    df['AverageMonthlyRate'] = df['MonthlyRate'] / df['YearsAtCompany']\n",
    "    df['JobRoleTenure'] = df['YearsInCurrentRole'] / df['YearsAtCompany']\n",
    "    df['PromotionFrequency'] = df['YearsSinceLastPromotion'] / df['YearsAtCompany']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function on dataset\n",
    "NEW_TRAINING = feature_extraction(TRAINING)\n",
    "NEW_TESTING = feature_extraction(TESTING)\n",
    "NEW_AUGMENTED = feature_extraction(AUGMENTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on categorical features\n",
    "NEW_TRAINING = pd.get_dummies(NEW_TRAINING, columns=CATEGORICALS)\n",
    "NEW_TESTING = pd.get_dummies(NEW_TESTING, columns=CATEGORICALS)\n",
    "NEW_AUGMENTED = pd.get_dummies(NEW_AUGMENTED, columns=CATEGORICALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_TRAINING = NEW_TRAINING.dropna()\n",
    "NEW_AUGMENTED = NEW_AUGMENTED.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, mean_squared_error\n",
    "import tensorflow as ts\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers  import Adam, SGD\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', activation='relu', hidden_units=64):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(hidden_units, activation=activation))\n",
    "    model.add(Dense(hidden_units, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    # 'dropout_rate': [0.2, 0.3],\n",
    "    'model__hidden_units': [32, 64, 128]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifier\n",
    "model = KerasClassifier(model= create_model, verbose = 0)\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1...\n",
      "Processing fold 2...\n",
      "Processing fold 3...\n",
      "Processing fold 4...\n",
      "Processing fold 5...\n"
     ]
    }
   ],
   "source": [
    "# Perform K-fold stratified sampling and augment the data\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(NEW_TRAINING, NEW_TRAINING['Attrition'])):\n",
    "    print(f\"Processing fold {fold + 1}...\")\n",
    "    # Split the training data into train and validation sets\n",
    "    train_fold = NEW_TRAINING.iloc[train_index]\n",
    "    val_fold = NEW_TRAINING.iloc[test_index]\n",
    "    \n",
    "    # Augment the training data with the augmentation data\n",
    "    train_fold_augmented = pd.concat([train_fold, NEW_AUGMENTED], ignore_index=True)\n",
    "    \n",
    "    X_train = train_fold_augmented.drop('Attrition', axis=1)\n",
    "    y_train = train_fold_augmented['Attrition']\n",
    "    X_val = val_fold.drop('Attrition', axis=1)\n",
    "    y_val = val_fold['Attrition']\n",
    "    \n",
    "       \n",
    "    # Perform grid search\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc', cv=skf)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_result.best_estimator_\n",
    "    \n",
    "    # Evaluate the best model on the validation set\n",
    "    y_pred_val = best_model.predict(X_val)\n",
    "    y_pred_proba_val = best_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    auc_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "    f1_val = f1_score(y_val, y_pred_val)\n",
    "    accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "    mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "    \n",
    "    results.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Model': 'Neural Network',\n",
    "        'Best Hyperparameters': grid_result.best_params_,\n",
    "        'AUC': auc_val,\n",
    "        'F1': f1_val,\n",
    "        'Accuracy': accuracy_val,\n",
    "        'MSE': mse_val\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fold           Model                               Best Hyperparameters  \\\n",
      "0     1  Neural Network  {'model__activation': 'tanh', 'model__hidden_u...   \n",
      "1     2  Neural Network  {'model__activation': 'tanh', 'model__hidden_u...   \n",
      "2     3  Neural Network  {'model__activation': 'tanh', 'model__hidden_u...   \n",
      "3     4  Neural Network  {'model__activation': 'tanh', 'model__hidden_u...   \n",
      "4     5  Neural Network  {'model__activation': 'tanh', 'model__hidden_u...   \n",
      "\n",
      "        AUC   F1  Accuracy       MSE  \n",
      "0  0.682286  0.0  0.889231  0.110769  \n",
      "1  0.602172  0.0  0.889231  0.110769  \n",
      "2  0.565785  0.0  0.886154  0.113846  \n",
      "3  0.698929  0.0  0.888889  0.111111  \n",
      "4  0.644097  0.0  0.888889  0.111111  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model on the entire training data using the best hyperparameters from the best fold\n",
    "best_params = results_df.loc[results_df['AUC'].idxmax(), 'Best Hyperparameters']\n",
    "full_train_data = pd.concat([NEW_TRAINING, NEW_AUGMENTED], ignore_index=True)\n",
    "X_full_train = full_train_data.drop('Attrition', axis=1)\n",
    "y_full_train = full_train_data['Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the final model\n",
    "final_model = create_model(learning_rate=best_params['learning_rate'], dropout_rate=best_params['dropout_rate'])\n",
    "final_model.fit(X_full_train, y_full_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "y_pred_proba_test = final_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Combine the id column with the predicted probabilities and save as CSV\n",
    "id_column = test['id']\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': id_column,\n",
    "    'predicted_probability': y_pred_proba_test\n",
    "})\n",
    "\n",
    "predictions_df.to_csv('neural_network_predictions.csv', index=False)\n",
    "print(\"Predictions saved to neural_network_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
